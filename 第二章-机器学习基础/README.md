### 机器学习基础概念

#### 关于数据

鸢尾花的数据为例：描述萼片长宽、花瓣长宽，所属的鸢尾花的类别。

数据的整体叫**数据集（dataset）**。

每一行数据称为一个**样本**。

除了标识类别的列外，每一列表达样本的一个**特征**。

标识类别的列称为**标记（label）**。

数据集中包含表示特征的那些列的一行数据可以用一个**特征向量**来表示，一般使用列向量进行表示。

特征空间（feature space），每一个样本都是在其空间中的一个点。

分类任务本质就是在特征空间切分。

特征可以很抽象，比如说图像，每一个像素点都是一个特征，28*28的图像有784个特征（MNIST数据集），彩色图像的特征会更多。

#### 机器学习的基本任务

监督学习：

- 分类任务
- 回归任务

分类任务很好理解，如图像识别，手写数字识别。分类任务进一步细分可以分为：二分类任务（两个类别中选择一个），多分类任务（多个类别，如手写数字识别）。很多问题都可以解决分类问题。有些算法只支持完成二分类任务，*但是多分类任务可以转换为二分类任务*。

多标签分类，比如一张图片贴上多个标签（比较前沿）。

回归任务，结果是一个连续的数字，而非一个类别，比如说预测房屋价格，市场分析。某些算法只能解决回归，某些算法只能解决分类，有时候某些算法都可以解决。在一些情况下，回归任务可以简化为分类任务，如预测学生成绩，不要求具体的数值，要求预测到某个类别。

所以机器学习就是通过输入大量的学习资料，通过机器学习算法，生成一个模型（其实就是一个函数），之后输入新的样例我们就可以得到一个结果。

#### 机器学习的算法的分类

**监督学习**、**非监督学习**、**半监督学习**和**增强学习**

##### 监督学习

给机器学习的训练数据拥有“标记”或者“答案”。比如图像已经拥有了标记信息，银行已经积累了一定的客户信息和他们信用卡的信用情况，医院已经积累了一定的病人信息和他们最终确诊是否患病的情况，市场已经积累了房屋的基本信息和最终成交的金额。*监督学习处理分类和回归问题。*

##### 非监督学习

给机器学习的训练数据没有“标记”或者“答案”，对没有“标记”的数据进行分类 - 聚类分析。非监督学习的意义：对数据进行降维处理，特征提取（信用卡的信用评级和人的胖瘦无关），特征压缩（PCA）尽量减少信息损失的情况下，将高维数据降维为地位数据，降维的处理的意义：方便我们进行可视化。除此之外，非监督学习还可以帮助我们进行异常检测。

#### 半监督学习

一部分数据有“标记”或者“答案”，另一部分数据没有。

其实，半监督学习更常见：各种原因产生的标记缺失。通常都会使用无监督学习的手段对数据做处理，之后使用监督学习的手段做模型的训练和预测。

#### 增强学习

根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式，alphaGO，无人驾驶，机器人。

#### 另外的机器学习的分类方法

##### 在线学习和批量学习（离线学习）

Batch learning，Online learning。

**批量学习**：机器学习算法通过接收输入大量学习资料，训练出模型，模型之后不会再改变，尽管会不断接收新的参数。

优点：简单。问题：怎么去适应环境的变化？解决方案就是定时去重新进行批量学习。缺点就是每次重新批量学习时，运算量巨大，对某些环境变化很快的情况无法适应。

**在线学习**：输入样例不会直接扔掉，而会重新作为机器学习算法的输入作用于模型生成。

优点：可以及时地反映环境的变化。

问题：新的数据会带来不好的变化。所以需要加强对数据的监控。其他情况：也适用于数据量巨大，完全无法批量学习的环境。

##### 参数学习和非参数学习

参数学习的特点：一旦学习到了参数，就不再需要原有的数据集。

非参数学习：不对模型进行过多的假设，非参数学习不等于没参数。

#### 一些思考

- 数据非常重要
- 数据驱动
- 算法也很重要
- 奥卡姆剃刀（简单的就是好的）
- 具体到某个特定问题，有些算法可能更好
- 脱离具体的问题，谈那个算法好是没有意义的
- 面对不确定的问题，如何看待机器学习的预测结果？